from typing import Any, Type, TypeVar, cast
import torch
from accelerate.utils import send_to_device
from torch import Tensor, nn
from transformers import PreTrainedModel

T = TypeVar("T")

def assert_type(typ: Type[T], obj: Any) -> T:
    if not isinstance(obj, typ):
        raise TypeError(f"Expected {typ.__name__}, got {type(obj).__name__}")

    return cast(typ, obj)

@torch.no_grad()
def geometric_median(points: Tensor, max_iter: int = 100, tol: float = 1e-5):
    guess = points.mean(dim=0)
    prev = torch.zeros_like(guess)

    weights = torch.ones(len(points), device=points.device)

    for _ in range(max_iter):
        prev = guess

        weights = 1 / torch.norm(points - guess, dim=1)

        weights /= weights.sum()

        guess = (weights.unsqueeze(1) * points).sum(dim=0)

        if torch.norm(guess - prev) < tol:
            break

    return guess

def get_layer_list(model: PreTrainedModel) -> tuple[str, nn.ModuleList]:
    N = assert_type(int, model.config.num_hidden_layers)
    candidates = [
        (name, mod)
        for (name, mod) in model.named_modules()
        if isinstance(mod, nn.ModuleList) and len(mod) == N
    ]
    assert len(candidates) == 1, "Could not find the list of layers."

    return candidates[0]

@torch.inference_mode()
def resolve_width(
    model: PreTrainedModel, 
    module_name: str, 
    dim: int = -1,
) -> dict[str, int]:
    
    module_to_resolve = model.get_submodule(module_name)
    shape = list()

    def hook(module, _, output):
        if isinstance(output, tuple):
            output, *_ = output
        shape.append(output.shape[dim])
    
    handle = module_to_resolve.register_forward_hook(hook)
    dummy = send_to_device(model.dummy_inputs, model.device)
    model(**dummy)
    handle.remove()
    
    return shape[0]

@torch.inference_mode()
def resolve_widths(
    model: PreTrainedModel, module_names: list[str], dim: int = -1,
) -> dict[str, int]:
    module_to_name = {
        model.get_submodule(name): name for name in module_names
    }
    shapes: dict[str, int] = {}

    def hook(module, _, output):
        if isinstance(output, tuple):
            output, *_ = output

        name = module_to_name[module]
        shapes[name] = output.shape[dim]

    handles = [
        mod.register_forward_hook(hook) for mod in module_to_name
    ]
    dummy = send_to_device(model.dummy_inputs, model.device)
    try:
        model(**dummy)
    finally:
        for handle in handles:
            handle.remove()
    
    return shapes